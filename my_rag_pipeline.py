# -*- coding: utf-8 -*-
"""LionOChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mdN6IytbbrjbKIZkCHpPnGqBl74Vtth3
"""


from langchain_community.document_loaders import PyPDFLoader


from langchain_community.document_loaders import WebBaseLoader
import bs4


from langchain.document_loaders import UnstructuredWordDocumentLoader

#Load webpages
web_loader = WebBaseLoader([
    "https://www.lionobytes.com/in/products/crm/overview",
    "https://www.lionobytes.com/in/products/field-service-management/overview",
    "https://www.lionobytes.com/in/about-us"
])
web_docs = web_loader.load()

# Load PDF
pdf_paths = [
    "D:\Projects\LionO_Chatbot\FILEs\HistoryP.pdf",
]
pdf_docs = []
for path in pdf_paths:
    pdf_loader = PyPDFLoader(path)
    pdf_docs.extend(pdf_loader.load())

# Load DOCX
docx_paths = [
    "D:\Projects\LionO_Chatbot\FILEs\GeographyD.docx",
]
docx_docs = []
for path in docx_paths:
    docx_loader = UnstructuredWordDocumentLoader(path)
    docx_docs.extend(docx_loader.load())

# Combine all
all_docs = web_docs + pdf_docs + docx_docs

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
texts = text_splitter.split_documents(all_docs)


from langchain_huggingface import HuggingFaceEmbeddings

# Initialize the embedding model
embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")


from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(documents=texts, embedding=embeddings_model)

"""# --- Wrap Groq Chat LLM for LangChain ---"""


from langchain_groq import ChatGroq

from langchain.schema import HumanMessage
from langchain.llms.base import LLM

from langchain.chains.summarize import load_summarize_chain
from langchain.chains.combine_documents.base import BaseCombineDocumentsChain
from typing import Optional, List
from pydantic import Field

class ChatGroqWrapper(LLM):
    chat_groq_llm: ChatGroq = Field(...)

    @property
    def _llm_type(self) -> str:
        return "chatgroq"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        messages = [HumanMessage(content=prompt)]
        response = self.chat_groq_llm(messages)
        return response.content


from dotenv import load_dotenv
import os

# Load the .env file
load_dotenv()  
groq_api_key = os.getenv("GROQ_API_KEY")

# Initialize ChatGroq LLM
groq_llm = ChatGroq(
    api_key=groq_api_key,
    model="openai/gpt-oss-20b"
)

wrapped_llm = ChatGroqWrapper(chat_groq_llm=groq_llm)

combine_documents_chain: BaseCombineDocumentsChain = load_summarize_chain(
    llm=wrapped_llm,
    chain_type="map_reduce",
    verbose=True
)

# ---Create Retriever ---
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

custom_prompt = PromptTemplate(
    template="""
You are a helpful assistant.
Use ONLY the information in the provided context to answer the question.
If the answer is not in the context, say "Answer not found in the provided content."

Context:
{context}

Question:
{question}

Answer:
""",
    input_variables=["context", "question"],
)

# --- Build RetrievalQA Chain ---
qa_chain = RetrievalQA.from_chain_type(
    llm=wrapped_llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt},
    return_source_documents=True
)

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

conversational_qa_chain = RunnableWithMessageHistory(
    qa_chain,
    get_session_history,
    input_messages_key="query",     # RetrievalQA expects "query"
    history_messages_key="chat_history",
    output_messages_key="result",  # RetrievalQA outputs in "result"
)

response = conversational_qa_chain.invoke(
    {"query": "Sultanate Period Architecture"},
    config={"configurable": {"session_id": "abc123"}}
)
print("Answer:", response["result"])

from langchain_core.messages import AIMessage

for message in store["abc123"].messages:
    if isinstance(message, AIMessage):
        prefix = "AI"
    else:
        prefix = "User"

    print(f"{prefix}: {message.content}\n")

